// @ts-nocheck
import "https://deno.land/x/xhr@0.1.0/mod.ts";
import { serve } from "https://deno.land/std@0.168.0/http/server.ts";
import { authenticateRequest } from '../_shared/auth-middleware.ts';
import { enforceEdgeRateLimit, buildRateLimitResponse } from '../_shared/rate-limit.ts';
import { validateRequest, createErrorResponse, VoiceAnalysisSchema } from '../_shared/validation.ts';

const corsHeaders = {
  'Access-Control-Allow-Origin': '*',
  'Access-Control-Allow-Headers': 'authorization, x-client-info, apikey, content-type',
};

serve(async (req) => {
  if (req.method === 'OPTIONS') {
    return new Response(null, { headers: corsHeaders });
  }

  try {
    // üîí S√âCURIT√â: Authentification obligatoire
    const authResult = await authenticateRequest(req);
    if (authResult.status !== 200 || !authResult.user) {
      console.warn('[analyze-voice-hume] Unauthorized access attempt');
      return createErrorResponse(authResult.error || 'Authentication required', authResult.status, corsHeaders);
    }

    // üõ°Ô∏è S√âCURIT√â: Rate limiting strict (10 req/min - Whisper + Lovable AI)
    const rateLimit = await enforceEdgeRateLimit(req, {
      route: 'analyze-voice-hume',
      userId: authResult.user.id,
      limit: 10,
      windowMs: 60_000,
      description: 'Voice analysis - OpenAI Whisper + Lovable AI'
    });

    if (!rateLimit.allowed) {
      console.warn('[analyze-voice-hume] Rate limit exceeded', { userId: authResult.user.id });
      return buildRateLimitResponse(rateLimit, corsHeaders, {
        errorCode: 'rate_limit_exceeded',
        message: `Trop de requ√™tes vocales. R√©essayez dans ${rateLimit.retryAfterSeconds}s.`
      });
    }

    // ‚úÖ VALIDATION: Validation Zod des entr√©es
    const validation = await validateRequest(req, VoiceAnalysisSchema);
    if (!validation.success) {
      return createErrorResponse(validation.error, validation.status, corsHeaders);
    }

    const { audioBase64 } = validation.data;

    const LOVABLE_API_KEY = Deno.env.get('LOVABLE_API_KEY');
    const HUME_API_KEY = Deno.env.get('HUME_API_KEY');
    
    if (!LOVABLE_API_KEY) {
      console.log('[analyze-voice-hume] LOVABLE_API_KEY not configured');
      throw new Error('LOVABLE_API_KEY not configured');
    }

    const startTime = Date.now();

    // Extraire les donn√©es audio base64
    let audioData = audioBase64;
    if (audioData.includes(',')) {
      audioData = audioData.split(',')[1];
    }

    // √âtape 1: Transcription audio avec Lovable AI
    let transcript = '';
    
    console.log('[analyze-voice-hume] Transcribing audio with Lovable AI');
    
    try {
      const transcriptionResponse = await fetch('https://ai.gateway.lovable.dev/v1/chat/completions', {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${LOVABLE_API_KEY}`,
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          model: 'google/gemini-2.5-flash',
          messages: [
            {
              role: 'system',
              content: 'Tu es un expert en transcription audio. Transcris fid√®lement le contenu audio fourni en fran√ßais. Retourne uniquement le texte transcrit, sans commentaires ni annotations.'
            },
            {
              role: 'user',
              content: [
                {
                  type: 'text',
                  text: 'Transcris cet enregistrement audio en fran√ßais :'
                },
                {
                  type: 'input_audio',
                  input_audio: {
                    data: audioData,
                    format: 'webm'
                  }
                }
              ]
            }
          ]
        })
      });

      if (transcriptionResponse.ok) {
        const transcriptionData = await transcriptionResponse.json();
        transcript = transcriptionData.choices?.[0]?.message?.content?.trim() || '';
        console.log('[analyze-voice-hume] Transcription:', transcript.substring(0, 100));
      } else {
        console.warn('[analyze-voice-hume] Transcription failed:', await transcriptionResponse.text());
      }
    } catch (e) {
      console.warn('[analyze-voice-hume] Transcription error:', e);
    }

    // Si pas de transcription, retourner un √©tat par d√©faut
    if (!transcript || transcript.length < 3) {
      console.log('[analyze-voice-hume] No valid transcript, returning neutral state');
      return new Response(
        JSON.stringify({
          emotion: 'neutre',
          valence: 0.5,
          arousal: 0.5,
          confidence: 0.3,
          emotions: { 'neutre': 0.3 },
          transcript: '',
          latency_ms: Date.now() - startTime,
          note: 'Transcription audio non d√©tect√©e. Veuillez parler plus clairement ou plus longtemps.'
        }),
        { headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
      );
    }

    console.log('[analyze-voice-hume] Using Lovable AI for emotion analysis');
    
    const analysisResponse = await fetch('https://ai.gateway.lovable.dev/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${LOVABLE_API_KEY}`,
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: 'google/gemini-2.5-flash',
        messages: [
          {
            role: 'system',
            content: `Tu es un expert en psychologie et en analyse √©motionnelle. Ton r√¥le est d'analyser le contenu √©motionnel d'un texte transcrit √† partir d'un enregistrement vocal.

IMPORTANT: Analyse attentivement le CONTENU S√âMANTIQUE du texte, pas seulement les mots.
- Si la personne dit qu'elle est stress√©e, anxieuse, ou exprime du stress ‚Üí d√©tecte "anxi√©t√©" ou "stress" avec une valence BASSE (0.2-0.35) et arousal √âLEV√â (0.65-0.85)
- Si la personne exprime de la tristesse, du mal-√™tre ‚Üí d√©tecte "tristesse" avec valence TR√àS BASSE (0.1-0.25)
- Si la personne exprime de la joie, du bonheur ‚Üí d√©tecte "joie" avec valence HAUTE (0.75-0.95)
- Le stress et l'anxi√©t√© sont des √©motions N√âGATIVES √† haute √©nergie

Ne r√©ponds JAMAIS "neutre" si la personne exprime clairement une √©motion. √âcoute le sens de ses mots.`
          },
          {
            role: 'user',
            content: `Voici la transcription de ce que la personne a dit: "${transcript}"\n\nAnalyse l'√©motion dominante exprim√©e.`
          }
        ],
        tools: [
          {
            type: 'function',
            function: {
              name: 'analyze_voice_emotion',
              description: 'Analyse les √©motions dans la transcription vocale',
              parameters: {
                type: 'object',
                properties: {
                  emotion: {
                    type: 'string',
                    description: '√âmotion principale d√©tect√©e',
                    enum: ['joie', 'tristesse', 'col√®re', 'peur', 'surprise', 'd√©go√ªt', 'anxi√©t√©', 'stress', 'calme', 'excitation', 'confiance', 'neutre', 'frustration', 's√©r√©nit√©', 'espoir', 'm√©lancolie', 'satisfaction', 'inqui√©tude', 'fatigue', 'ennui', 'nervosit√©', 'tension']
                  },
                  valence: {
                    type: 'number',
                    description: 'Valence √©motionnelle (0=n√©gatif, 1=positif)',
                    minimum: 0,
                    maximum: 1
                  },
                  arousal: {
                    type: 'number',
                    description: 'Niveau d\'activation (0=calme, 1=excit√©)',
                    minimum: 0,
                    maximum: 1
                  },
                  confidence: {
                    type: 'number',
                    description: 'Niveau de confiance de l\'analyse',
                    minimum: 0,
                    maximum: 1
                  },
                  secondary_emotions: {
                    type: 'object',
                    description: 'Top 3 √©motions secondaires avec leurs scores',
                    additionalProperties: {
                      type: 'number',
                      minimum: 0,
                      maximum: 1
                    }
                  }
                },
                required: ['emotion', 'valence', 'arousal', 'confidence', 'secondary_emotions'],
                additionalProperties: false
              }
            }
          }
        ],
        tool_choice: { type: 'function', function: { name: 'analyze_voice_emotion' } }
      })
    });

    if (!analysisResponse.ok) {
      if (analysisResponse.status === 429) {
        throw new Error('Limite de requ√™tes Lovable AI atteinte. Veuillez r√©essayer plus tard.');
      }
      if (analysisResponse.status === 402) {
        throw new Error('Cr√©dits Lovable AI insuffisants. Veuillez recharger votre compte.');
      }
      const error = await analysisResponse.text();
      console.error('[analyze-voice-hume] Lovable AI error:', error);
      throw new Error(`Lovable AI error: ${analysisResponse.status}`);
    }

    const analysisData = await analysisResponse.json();
    const toolCall = analysisData.choices[0]?.message?.tool_calls?.[0];
    
    if (!toolCall) {
      throw new Error('No tool call in Lovable AI response');
    }

    const analysisResult = JSON.parse(toolCall.function.arguments);
    
    const latencyMs = Date.now() - startTime;

    const result = {
      emotion: analysisResult.emotion,
      valence: analysisResult.valence,
      arousal: analysisResult.arousal,
      confidence: analysisResult.confidence,
      emotions: {
        [analysisResult.emotion]: analysisResult.confidence,
        ...analysisResult.secondary_emotions
      },
      transcript,
      latency_ms: latencyMs
    };

    console.log('[analyze-voice-hume] Success:', JSON.stringify(result));

    return new Response(
      JSON.stringify(result),
      { headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
    );

  } catch (error) {
    console.error('[analyze-voice-hume] Error:', error);
    return new Response(
      JSON.stringify({ 
        error: error instanceof Error ? error.message : 'Unknown error',
        details: 'Failed to analyze voice'
      }),
      { 
        status: 500,
        headers: { ...corsHeaders, 'Content-Type': 'application/json' }
      }
    );
  }
});
