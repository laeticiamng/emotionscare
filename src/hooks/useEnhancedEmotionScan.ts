import { useState, useCallback, useRef, useEffect } from 'react';
import { EmotionResult, ScanMode, EmotionAnalysisConfig, BiometricData, EmotionConfidence, EmotionVector } from '@/types/emotion';
import { emotionsCareApi } from '@/services/emotionsCareApi';
import { useLogger } from '@/lib/logger';

interface UseEnhancedEmotionScanReturn {
  isScanning: boolean;
  scanProgress: number;
  currentResult: EmotionResult | null;
  permissions: {
    camera: boolean;
    microphone: boolean;
  };
  scanHistory: EmotionResult[];
  startScan: (mode: ScanMode, data?: any) => Promise<void>;
  stopScan: () => void;
  resetScan: () => void;
  updateConfig: (config: EmotionAnalysisConfig) => void;
  getRealtimeEmotion: () => EmotionResult | null;
}

export const useEnhancedEmotionScan = (initialConfig: EmotionAnalysisConfig): UseEnhancedEmotionScanReturn => {
  const logger = useLogger();
  
  // États principaux
  const [isScanning, setIsScanning] = useState(false);
  const [scanProgress, setScanProgress] = useState(0);
  const [currentResult, setCurrentResult] = useState<EmotionResult | null>(null);
  const [scanHistory, setScanHistory] = useState<EmotionResult[]>([]);
  const [config, setConfig] = useState(initialConfig);
  const [permissions, setPermissions] = useState({
    camera: false,
    microphone: false
  });

  // Refs pour les streams et intervalles
  const mediaStreamRef = useRef<MediaStream | null>(null);
  const scanIntervalRef = useRef<NodeJS.Timeout | null>(null);
  const progressIntervalRef = useRef<NodeJS.Timeout | null>(null);
  const sessionIdRef = useRef<string | null>(null);

  // Vérifier les permissions au chargement
  useEffect(() => {
    checkPermissions();
  }, []);

  // Nettoyer les ressources au démontage
  useEffect(() => {
    return () => {
      stopAllStreams();
      clearAllIntervals();
    };
  }, []);

  const checkPermissions = async () => {
    try {
      // Vérifier la caméra
      try {
        const videoStream = await navigator.mediaDevices.getUserMedia({ video: true });
        videoStream.getTracks().forEach(track => track.stop());
        setPermissions(prev => ({ ...prev, camera: true }));
      } catch (error) {
        logger.warn('Camera permission denied', { error });
        setPermissions(prev => ({ ...prev, camera: false }));
      }

      // Vérifier le microphone
      try {
        const audioStream = await navigator.mediaDevices.getUserMedia({ audio: true });
        audioStream.getTracks().forEach(track => track.stop());
        setPermissions(prev => ({ ...prev, microphone: true }));
      } catch (error) {
        logger.warn('Microphone permission denied', { error });
        setPermissions(prev => ({ ...prev, microphone: false }));
      }
    } catch (error) {
      logger.error('Error checking permissions', { error });
    }
  };

  const stopAllStreams = () => {
    if (mediaStreamRef.current) {
      mediaStreamRef.current.getTracks().forEach(track => track.stop());
      mediaStreamRef.current = null;
    }
  };

  const clearAllIntervals = () => {
    if (scanIntervalRef.current) {
      clearInterval(scanIntervalRef.current);
      scanIntervalRef.current = null;
    }
    if (progressIntervalRef.current) {
      clearInterval(progressIntervalRef.current);
      progressIntervalRef.current = null;
    }
  };

  const generateSessionId = () => {
    return `scan_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
  };

  const simulateBiometrics = (): BiometricData => {
    return {
      heartRate: 60 + Math.random() * 40,
      breathingRate: 12 + Math.random() * 8,
      skinConductance: Math.random() * 100,
      eyeTracking: {
        gazeDirection: { x: Math.random() * 2 - 1, y: Math.random() * 2 - 1 },
        blinkRate: 15 + Math.random() * 10,
        pupilDilation: 3 + Math.random() * 2
      },
      faceMetrics: {
        expressionIntensity: Math.random(),
        microExpressions: ['subtle_smile', 'brow_furrow'].filter(() => Math.random() > 0.7),
        faceOrientation: {
          pitch: (Math.random() - 0.5) * 30,
          yaw: (Math.random() - 0.5) * 60,
          roll: (Math.random() - 0.5) * 20
        }
      }
    };
  };

  const generateEmotionVector = (emotion: string): EmotionVector => {
    // Mappage simplifié des émotions aux dimensions VAD
    const emotionMap: Record<string, EmotionVector> = {
      happy: { valence: 0.8, arousal: 0.6, dominance: 0.7 },
      sad: { valence: -0.7, arousal: 0.3, dominance: 0.2 },
      angry: { valence: -0.6, arousal: 0.9, dominance: 0.8 },
      fear: { valence: -0.8, arousal: 0.8, dominance: 0.1 },
      surprise: { valence: 0.2, arousal: 0.8, dominance: 0.5 },
      disgust: { valence: -0.7, arousal: 0.4, dominance: 0.6 },
      calm: { valence: 0.3, arousal: 0.1, dominance: 0.6 },
      excited: { valence: 0.7, arousal: 0.9, dominance: 0.8 }
    };

    return emotionMap[emotion.toLowerCase()] || { valence: 0, arousal: 0.5, dominance: 0.5 };
  };

  const analyzeEmotion = async (mode: ScanMode, inputData?: any): Promise<EmotionResult> => {
    try {
      logger.info('Starting emotion analysis', { mode, config });

      let analysisResult: any;
      
      switch (mode) {
        case 'text':
          analysisResult = await emotionsCareApi.analyzeEmotion(inputData.text);
          break;
        case 'voice':
          analysisResult = await emotionsCareApi.analyzeVoiceEmotion(inputData.audioBlob);
          break;
        case 'facial':
        case 'combined':
        case 'realtime':
          // Pour la démo, on simule l'analyse
          await new Promise(resolve => setTimeout(resolve, 1000));
          analysisResult = {
            emotion: ['happy', 'calm', 'focused', 'excited', 'neutral'][Math.floor(Math.random() * 5)],
            confidence: 85 + Math.random() * 15
          };
          break;
        default:
          throw new Error(`Mode de scan non supporté: ${mode}`);
      }

      // Construction du résultat enrichi
      const emotion = analysisResult.emotion || 'neutral';
      const baseConfidence = analysisResult.confidence || (70 + Math.random() * 25);
      
      const confidence: EmotionConfidence = {
        overall: baseConfidence,
        facial: mode.includes('facial') ? baseConfidence + Math.random() * 5 : undefined,
        vocal: mode.includes('voice') ? baseConfidence + Math.random() * 5 : undefined,
        textual: mode === 'text' ? baseConfidence + Math.random() * 5 : undefined,
        temporal: config.realTimeUpdates ? baseConfidence * 0.95 : undefined
      };

      const result: EmotionResult = {
        id: `emotion_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`,
        emotion,
        confidence,
        timestamp: new Date(),
        source: mode === 'text' ? 'text_analysis' : 
                mode === 'voice' ? 'voice_analysis' : 
                mode === 'combined' ? 'multimodal' : 'facial_analysis',
        scanMode: mode,
        duration: config.duration * 1000,
        vector: generateEmotionVector(emotion),
        biometrics: config.biometricTracking ? simulateBiometrics() : undefined,
        rawData: analysisResult,
        sessionId: sessionIdRef.current || undefined,
        details: {
          timeOfDay: new Date().getHours() < 12 ? 'morning' : 
                   new Date().getHours() < 17 ? 'afternoon' : 'evening',
          environment: 'home' // Détection automatique possible
        },
        predictions: config.predictiveMode ? {
          nextEmotionLikely: ['calm', 'focused'][Math.floor(Math.random() * 2)],
          stabilityScore: Math.random() * 100,
          recommendedActions: [
            'Prenez une pause de 5 minutes',
            'Pratiquez la respiration profonde',
            'Écoutez de la musique relaxante'
          ].slice(0, 1 + Math.floor(Math.random() * 2))
        } : undefined
      };

      logger.info('Emotion analysis completed', { result });
      return result;

    } catch (error) {
      logger.error('Error during emotion analysis', { error, mode });
      throw error;
    }
  };

  const setupMediaStream = async (mode: ScanMode): Promise<MediaStream | null> => {
    try {
      let constraints: MediaStreamConstraints = {};

      if (mode === 'facial' || mode === 'combined' || mode === 'realtime') {
        constraints.video = {
          facingMode: 'user',
          width: { ideal: 1280 },
          height: { ideal: 720 }
        };
      }

      if (mode === 'voice' || mode === 'combined' || mode === 'realtime') {
        constraints.audio = {
          echoCancellation: true,
          noiseSuppression: config.noiseReduction,
          autoGainControl: true
        };
      }

      if (Object.keys(constraints).length > 0) {
        const stream = await navigator.mediaDevices.getUserMedia(constraints);
        mediaStreamRef.current = stream;
        return stream;
      }

      return null;
    } catch (error) {
      logger.error('Error setting up media stream', { error, mode });
      throw new Error('Impossible d\'accéder aux médias requis');
    }
  };

  const startProgressTracking = () => {
    setScanProgress(0);
    const totalDuration = config.duration * 1000; // en millisecondes
    const updateInterval = 100; // mise à jour toutes les 100ms
    let elapsed = 0;

    progressIntervalRef.current = setInterval(() => {
      elapsed += updateInterval;
      const progress = Math.min((elapsed / totalDuration) * 100, 100);
      setScanProgress(progress);

      if (progress >= 100) {
        clearInterval(progressIntervalRef.current!);
        progressIntervalRef.current = null;
      }
    }, updateInterval);
  };

  const startScan = useCallback(async (mode: ScanMode, data?: any) => {
    try {
      logger.info('Starting scan', { mode, config });

      setIsScanning(true);
      setScanProgress(0);
      setCurrentResult(null);
      sessionIdRef.current = generateSessionId();

      // Configuration du stream média si nécessaire
      if (mode !== 'text') {
        await setupMediaStream(mode);
      }

      // Démarrer le suivi du progrès
      startProgressTracking();

      // Démarrer l'analyse
      setTimeout(async () => {
        try {
          const result = await analyzeEmotion(mode, data);
          setCurrentResult(result);
          setScanHistory(prev => [result, ...prev.slice(0, 19)]); // Garder les 20 derniers
          
          logger.info('Scan completed successfully', { result });
        } catch (error) {
          logger.error('Scan analysis failed', { error });
        } finally {
          setIsScanning(false);
          stopAllStreams();
          clearAllIntervals();
        }
      }, config.duration * 1000);

    } catch (error) {
      logger.error('Error starting scan', { error });
      setIsScanning(false);
      stopAllStreams();
      clearAllIntervals();
      throw error;
    }
  }, [config, logger]);

  const stopScan = useCallback(() => {
    logger.info('Stopping scan');
    
    setIsScanning(false);
    setScanProgress(0);
    stopAllStreams();
    clearAllIntervals();
    sessionIdRef.current = null;
  }, [logger]);

  const resetScan = useCallback(() => {
    logger.info('Resetting scan');
    
    setCurrentResult(null);
    setScanProgress(0);
    sessionIdRef.current = null;
  }, [logger]);

  const updateConfig = useCallback((newConfig: EmotionAnalysisConfig) => {
    logger.info('Updating scan config', { newConfig });
    setConfig(newConfig);
  }, [logger]);

  const getRealtimeEmotion = useCallback((): EmotionResult | null => {
    // Pour l'implémentation temps réel, retourner la dernière émotion détectée
    return currentResult;
  }, [currentResult]);

  return {
    isScanning,
    scanProgress,
    currentResult,
    permissions,
    scanHistory,
    startScan,
    stopScan,
    resetScan,
    updateConfig,
    getRealtimeEmotion
  };
};