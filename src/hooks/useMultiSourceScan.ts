/**
 * useMultiSourceScan - Hook pour combiner les analyses multi-sources
 * Fusionne les r√©sultats de scans texte, vocal et facial en un profil unifi√©
 */

import { useState, useCallback, useMemo } from 'react';
import { supabase } from '@/integrations/supabase/client';
import { useEmotionScan } from '@/hooks/useEmotionScan';
import { useToast } from '@/hooks/use-toast';
import { logger } from '@/lib/logger';

export interface MultiSourceResult {
  id: string;
  timestamp: string;
  sources: {
    text?: SourceResult;
    voice?: SourceResult;
    facial?: SourceResult;
  };
  fusion: {
    emotion: string;
    valence: number;
    arousal: number;
    confidence: number;
    dominance?: number;
  };
  insights: string[];
  reliability: 'high' | 'medium' | 'low';
}

export interface SourceResult {
  emotion: string;
  valence: number;
  arousal: number;
  confidence: number;
  timestamp: string;
}

interface FusionWeights {
  text: number;
  voice: number;
  facial: number;
}

const DEFAULT_WEIGHTS: FusionWeights = {
  text: 0.3,
  voice: 0.35,
  facial: 0.35
};

export function useMultiSourceScan() {
  const [isScanning, setIsScanning] = useState(false);
  const [results, setResults] = useState<MultiSourceResult | null>(null);
  const [progress, setProgress] = useState(0);
  const [activeSource, setActiveSource] = useState<'text' | 'voice' | 'facial' | null>(null);
  const { toast } = useToast();
  const { analyzeText, analyzeVoice, analyzeImage } = useEmotionScan();

  // Fusion des r√©sultats multi-sources avec pond√©ration
  const fuseResults = useCallback((
    sources: MultiSourceResult['sources'],
    weights: FusionWeights = DEFAULT_WEIGHTS
  ): MultiSourceResult['fusion'] => {
    const validSources = Object.entries(sources).filter(([_, v]) => v !== undefined);
    
    if (validSources.length === 0) {
      return {
        emotion: 'neutre',
        valence: 50,
        arousal: 50,
        confidence: 0
      };
    }

    // Normaliser les poids en fonction des sources disponibles
    const totalWeight = validSources.reduce((sum, [key]) => {
      return sum + weights[key as keyof FusionWeights];
    }, 0);

    let weightedValence = 0;
    let weightedArousal = 0;
    let weightedConfidence = 0;
    const emotionCounts: Record<string, number> = {};

    validSources.forEach(([key, source]) => {
      if (!source) return;
      const weight = weights[key as keyof FusionWeights] / totalWeight;
      
      weightedValence += source.valence * weight;
      weightedArousal += source.arousal * weight;
      weightedConfidence += source.confidence * weight;
      
      // Compter les √©motions pour le consensus
      const emotion = source.emotion.toLowerCase();
      emotionCounts[emotion] = (emotionCounts[emotion] || 0) + weight;
    });

    // D√©terminer l'√©motion dominante par consensus pond√©r√©
    const dominantEmotion = Object.entries(emotionCounts)
      .sort((a, b) => b[1] - a[1])[0]?.[0] || 'neutre';

    return {
      emotion: dominantEmotion,
      valence: Math.round(weightedValence),
      arousal: Math.round(weightedArousal),
      confidence: Math.round(weightedConfidence)
    };
  }, []);

  // G√©n√©rer des insights bas√©s sur la fusion
  const generateInsights = useCallback((
    sources: MultiSourceResult['sources'],
    fusion: MultiSourceResult['fusion']
  ): string[] => {
    const insights: string[] = [];
    const sourceCount = Object.values(sources).filter(Boolean).length;

    // Insight sur la coh√©rence
    if (sourceCount >= 2) {
      const valences = Object.values(sources)
        .filter(Boolean)
        .map(s => s!.valence);
      const variance = Math.abs(Math.max(...valences) - Math.min(...valences));
      
      if (variance < 15) {
        insights.push('‚úÖ Forte coh√©rence entre les diff√©rentes sources d\'analyse.');
      } else if (variance > 30) {
        insights.push('‚ö†Ô∏è Divergence d√©tect√©e entre les sources. L\'analyse textuelle et √©motionnelle peuvent diff√©rer.');
      }
    }

    // Insight sur l'√©tat √©motionnel
    if (fusion.valence > 70 && fusion.arousal > 60) {
      insights.push('üåü √âtat positif et √©nergique d√©tect√©. Profitez de ce moment!');
    } else if (fusion.valence < 30 && fusion.arousal > 60) {
      insights.push('üí™ Tension d√©tect√©e. Essayez un exercice de respiration.');
    } else if (fusion.valence > 60 && fusion.arousal < 40) {
      insights.push('üòå √âtat calme et serein. Excellent pour la r√©flexion.');
    } else if (fusion.valence < 40 && fusion.arousal < 40) {
      insights.push('üå± √ânergie basse. Une pause ou un peu de mouvement pourrait aider.');
    }

    // Insight sur la confiance
    if (fusion.confidence < 50) {
      insights.push('üìä Confiance mod√©r√©e. Essayez un scan avec plus de sources pour plus de pr√©cision.');
    } else if (fusion.confidence > 80) {
      insights.push('üéØ Analyse tr√®s fiable bas√©e sur plusieurs indicateurs concordants.');
    }

    return insights;
  }, []);

  // Calculer la fiabilit√©
  const calculateReliability = useCallback((
    sources: MultiSourceResult['sources'],
    fusion: MultiSourceResult['fusion']
  ): MultiSourceResult['reliability'] => {
    const sourceCount = Object.values(sources).filter(Boolean).length;
    
    if (sourceCount >= 3 && fusion.confidence > 70) return 'high';
    if (sourceCount >= 2 && fusion.confidence > 50) return 'medium';
    return 'low';
  }, []);

  // Scan multi-source complet
  const runMultiSourceScan = useCallback(async (inputs: {
    text?: string;
    audioBlob?: Blob;
    imageData?: string;
  }): Promise<MultiSourceResult | null> => {
    setIsScanning(true);
    setProgress(0);
    const sources: MultiSourceResult['sources'] = {};

    try {
      const totalSteps = Object.values(inputs).filter(Boolean).length;
      let completedSteps = 0;

      // Analyse textuelle
      if (inputs.text) {
        setActiveSource('text');
        try {
          const textResult = await analyzeText(inputs.text);
          sources.text = {
            emotion: textResult.emotion || 'neutre',
            valence: typeof textResult.valence === 'number' ? textResult.valence : 50,
            arousal: typeof textResult.arousal === 'number' ? textResult.arousal : 50,
            confidence: typeof textResult.confidence === 'number' ? textResult.confidence : 70,
            timestamp: new Date().toISOString()
          };
        } catch (err) {
          logger.warn('[useMultiSourceScan] Text analysis failed', {}, 'SCAN');
        }
        completedSteps++;
        setProgress((completedSteps / totalSteps) * 100);
      }

      // Analyse vocale
      if (inputs.audioBlob) {
        setActiveSource('voice');
        try {
          const voiceResult = await analyzeVoice(inputs.audioBlob);
          sources.voice = {
            emotion: voiceResult.emotion || 'neutre',
            valence: typeof voiceResult.valence === 'number' ? voiceResult.valence : 50,
            arousal: typeof voiceResult.arousal === 'number' ? voiceResult.arousal : 50,
            confidence: typeof voiceResult.confidence === 'number' ? voiceResult.confidence : 60,
            timestamp: new Date().toISOString()
          };
        } catch (err) {
          logger.warn('[useMultiSourceScan] Voice analysis failed', {}, 'SCAN');
        }
        completedSteps++;
        setProgress((completedSteps / totalSteps) * 100);
      }

      // Analyse faciale
      if (inputs.imageData) {
        setActiveSource('facial');
        try {
          const facialResult = await analyzeImage(inputs.imageData);
          sources.facial = {
            emotion: facialResult.emotion || 'neutre',
            valence: typeof facialResult.valence === 'number' ? facialResult.valence : 50,
            arousal: typeof facialResult.arousal === 'number' ? facialResult.arousal : 50,
            confidence: typeof facialResult.confidence === 'number' ? facialResult.confidence : 75,
            timestamp: new Date().toISOString()
          };
        } catch (err) {
          logger.warn('[useMultiSourceScan] Facial analysis failed', {}, 'SCAN');
        }
        completedSteps++;
        setProgress((completedSteps / totalSteps) * 100);
      }

      // V√©rifier qu'au moins une source a r√©ussi
      if (Object.values(sources).filter(Boolean).length === 0) {
        toast({
          title: 'Analyse √©chou√©e',
          description: 'Aucune source n\'a pu √™tre analys√©e.',
          variant: 'destructive'
        });
        return null;
      }

      // Fusionner les r√©sultats
      const fusion = fuseResults(sources);
      const insights = generateInsights(sources, fusion);
      const reliability = calculateReliability(sources, fusion);

      const result: MultiSourceResult = {
        id: crypto.randomUUID(),
        timestamp: new Date().toISOString(),
        sources,
        fusion,
        insights,
        reliability
      };

      setResults(result);

      // Sauvegarder dans clinical_signals
      const { data: { user } } = await supabase.auth.getUser();
      if (user) {
        await supabase.from('clinical_signals').insert({
          user_id: user.id,
          domain: 'emotional',
          level: Math.round(fusion.valence / 25),
          source_instrument: 'scan_multi',
          window_type: 'instant',
          module_context: 'scan',
          metadata: {
            sources: Object.keys(sources),
            fusion,
            insights,
            reliability
          },
          expires_at: new Date(Date.now() + 30 * 24 * 60 * 60 * 1000).toISOString()
        });
        
        window.dispatchEvent(new CustomEvent('scan-saved'));
      }

      toast({
        title: '‚úÖ Analyse multi-source termin√©e',
        description: `${Object.keys(sources).length} source(s) analys√©e(s) avec une fiabilit√© ${reliability === 'high' ? '√©lev√©e' : reliability === 'medium' ? 'moyenne' : 'faible'}.`
      });

      return result;

    } catch (error) {
      logger.error('[useMultiSourceScan] Error:', error, 'SCAN');
      toast({
        title: 'Erreur',
        description: 'Une erreur est survenue lors de l\'analyse.',
        variant: 'destructive'
      });
      return null;
    } finally {
      setIsScanning(false);
      setActiveSource(null);
      setProgress(100);
    }
  }, [analyzeText, analyzeVoice, analyzeImage, fuseResults, generateInsights, calculateReliability, toast]);

  // Statistiques sur les sources
  const sourceStats = useMemo(() => {
    if (!results) return null;
    
    const sources = results.sources;
    return {
      count: Object.values(sources).filter(Boolean).length,
      hasText: !!sources.text,
      hasVoice: !!sources.voice,
      hasFacial: !!sources.facial,
      coherence: (() => {
        const valences = Object.values(sources).filter(Boolean).map(s => s!.valence);
        if (valences.length < 2) return 100;
        return 100 - Math.abs(Math.max(...valences) - Math.min(...valences));
      })()
    };
  }, [results]);

  return {
    runMultiSourceScan,
    isScanning,
    results,
    progress,
    activeSource,
    sourceStats,
    reset: () => {
      setResults(null);
      setProgress(0);
      setActiveSource(null);
    }
  };
}

export default useMultiSourceScan;
